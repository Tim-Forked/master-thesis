{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import numpy as np\n",
    "import random\n",
    "import tank\n",
    "import truck\n",
    "\n",
    "import gym_pdsystem\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "import gym_pdsystem.utils.functions as fnc\n",
    "\n",
    "import os\n",
    "\n",
    "simulations_directory = './simulations'\n",
    "if not os.path.exists(simulations_directory):\n",
    "    os.makedirs(simulations_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COEFF = ct.COEFF\n",
    "\n",
    "C_TRANSPORT = ct.C_TRANSPORT\n",
    "C_LEVELS = ct.C_LEVELS\n",
    "\n",
    "p0_GLOBAL = ct.p0_GLOBAL\n",
    "\n",
    "P1_GLOBAL = ct.P1_GLOBAL\n",
    "P2_GLOBAL = ct.P2_GLOBAL\n",
    "\n",
    "M_GLOBAL = ct.M_GLOBAL\n",
    "\n",
    "NOT_DELIVERYING_PENALTY = P2_GLOBAL #to be equivalent/same importance as having 0 stock or surpassing max capacity levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_test_system(seed = None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Tanks' information\n",
    "    global n\n",
    "    n = 5 \n",
    "    tank_ids = list(range(1,n+1))\n",
    "    tank_max_loads =  np.array([100., 200, 100., 800., 200.])\n",
    "\n",
    "    tank_current_loads = np.full(n,0)\n",
    "    tank_consumption_rates =  np.array([5.] * n)\n",
    "    noisy_consumption_rate = True\n",
    "    \n",
    "    global n_discrete_load_levels\n",
    "    n_discrete_load_levels = np.array([4,4,4,4,4])\n",
    "    \n",
    "    load_level_percentages = np.array([ #b , c, e\n",
    "                                            [0.02, 0.31, 0.9],\n",
    "                                            [0.01, 0.03, 0.9],\n",
    "                                            [0.05, 0.16, 0.9],\n",
    "                                            [0.07, 0.14, 0.85],\n",
    "                                            [0.08, 0.26, 0.9]\n",
    "                                               ])\n",
    "        \n",
    "    for i, (lvl, max_load) in enumerate(zip(n_discrete_load_levels, tank_max_loads)):\n",
    "        a = np.linspace(0,max_load, lvl+1)[1]\n",
    "        current_load = np.random.randint(a+1,max_load)\n",
    "        tank_current_loads[i] = current_load     \n",
    "\n",
    "    # Trucks' information\n",
    "    global k\n",
    "    k = 2\n",
    "    truck_ids = list(range(k))\n",
    "    truck_max_loads = np.array([70.,130.])\n",
    "    truck_current_loads = truck_max_loads.copy()\n",
    "    truck_current_positions =  np.array([5] * k)\n",
    "    #truck_fractions_deliverable =  np.array([1.] * k) # we for now we only allow to deliver all the content of the truck\n",
    "    truck_fractions_deliverable =  np.array([ np.array([1.]), \n",
    "                                              np.array([1.])\n",
    "                                            ]) # we for now we only allow to deliver all the content of the truck\n",
    "    global n_discrete_load_levels_trucks\n",
    "    n_discrete_load_levels_trucks = np.array([1,1])\n",
    "\n",
    "    # System's information\n",
    "   \n",
    "    graph = ut.simple_graph(n+1)\n",
    "    tanks = [tank.Tank( tank_id, current_load, max_load, consumption_rate, n_lvls, d_lvls, noisy_consumption_rate) \n",
    "             for  tank_id, current_load, max_load, consumption_rate, n_lvls, d_lvls in \n",
    "             zip( tank_ids, tank_current_loads, tank_max_loads, tank_consumption_rates, n_discrete_load_levels,\n",
    "                  load_level_percentages)]\n",
    "    trucks = [truck.Truck( truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls) \n",
    "             for  truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls in \n",
    "             zip(truck_ids, truck_current_loads, truck_max_loads, truck_current_positions, \n",
    "                 truck_fractions_deliverable, n_discrete_load_levels_trucks)]\n",
    "\n",
    "    w =  np.array([32., 159., 162., 156.,156., 0.])\n",
    "\n",
    "    weights_matrix = ut.simple_weights(n+1, w)\n",
    "    \n",
    "    return(tanks, trucks, graph, weights_matrix)\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "# Action-State space dimension\n",
    "a_s_dim = toy_system.states_dim * toy_system.actions_dim\n",
    "a_s_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 143, 40, 307, 122]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1]]\n",
      "[[ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ 32. 159. 162. 156. 156.   0.]]\n"
     ]
    }
   ],
   "source": [
    "tanks, trucks, graph, weights_matrix = initialize_test_system(seed = 42)\n",
    "system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "print(system.tank_loads())\n",
    "print(system.graph)\n",
    "print(system.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitialize system function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reinitialize_system(system, seed = None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    for tank, tank_levels in zip(system.tanks, system.tanks_level):\n",
    "        a = tank_levels[0]\n",
    "        b = tank_levels[-1]\n",
    "        current_load = np.random.randint(a+1,b)*1.0\n",
    "        tank.load = current_load\n",
    "    system.reset_trucks_positions(); \n",
    "    return(system)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm (off-policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain parameters:\n",
    "retrain = False\n",
    "\n",
    "episode_retrain = 2*10**3\n",
    "retrain_episodes = 1*10**3\n",
    "\n",
    "\n",
    "# Train parameters:\n",
    "train_epsilon = True\n",
    "\n",
    "learning_rate0 = 1 #0.05\n",
    "learning_rate_decay = 0 #0.01\n",
    "\n",
    "episodes = 40*10**3 #episodes 40*\n",
    "episodes_epsilon_min = 10**2\n",
    "train_freq = 10**2 # 10**4\n",
    "train_vis_freq =  10**3\n",
    "train_rew_freq =  10**2\n",
    "train_Q_freq =  10**3\n",
    "episode_length = 30\n",
    "\n",
    "discount_rate = 1\n",
    "\n",
    "epsilon0 = 1.0\n",
    "epsilon_decay =( 1./(episodes_epsilon_min) ) \n",
    "epsilon_min = 0.05\n",
    "\n",
    "verbose = False\n",
    "verbose_info = False\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_visualization_steps = []\n",
    "train_rewards_list = []\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "Q = {}\n",
    "\n",
    "simulation_id = 4\n",
    "\n",
    "# Create directories for the simulations' outputs\n",
    "\n",
    "simulation_directory = './simulations/simulation{}'.format(simulation_id)\n",
    "if not os.path.exists(simulation_directory):\n",
    "    os.makedirs(simulation_directory)\n",
    "    os.makedirs(simulation_directory + '/Q-dictionaries')\n",
    "    os.makedirs(simulation_directory + '/discrewards')\n",
    "    os.makedirs(simulation_directory + '/vis')\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"The simulation id you tried to use has been already used before. Try to change it to a new one.\")\n",
    "\n",
    "    \n",
    "ut.save_obj(toy_system, simulation_directory+\"/system-sim\"+f\"{simulation_id}\")    \n",
    "\n",
    "# if retrain == True:\n",
    "#     simulation_id_retrain = 3\n",
    "#     iteration_retrain = 50*10**6\n",
    "#     Q = ut.load_obj(\"Q-dict-sim\" + f\"{simulation_id_retrain}\" + \"-\" + f\"{iteration_retrain}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def episodic_train_Q_epsilon( \n",
    "            epsilon0 = epsilon0,\n",
    "            epsilon_min = epsilon_min,\n",
    "            n_episodes = episodes, \n",
    "            episode_length = episode_length,\n",
    "            learning_rate0 = learning_rate0,\n",
    "            learning_rate_decay = learning_rate_decay,\n",
    "            discount_rate = discount_rate,\n",
    "            system = toy_system,\n",
    "            Q = Q, verbose = verbose, verbose_info = verbose_info,\n",
    "            visualization_steps = train_visualization_steps, rewards_list = train_rewards_list,\n",
    "            seed = seed, \n",
    "            freq = train_freq,\n",
    "            vis_freq = train_vis_freq,\n",
    "            rew_freq = train_rew_freq,\n",
    "            Q_freq = train_Q_freq,\n",
    "            simulation_id = simulation_id,\n",
    "            round_time = 2\n",
    "    \n",
    "           ):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    for episode in range(1,n_episodes+1):\n",
    "        print(\"\\rEpisode: {}\".format(episode), end=\"\")\n",
    "\n",
    "        reinitialize_system(system, seed = episode)\n",
    "        \n",
    "        ### epsilon-greedy exploration\n",
    "        epsilon = max( epsilon_min, epsilon0 / (1+(episode-1)*epsilon_decay) ) \n",
    "        \n",
    "        ### decrement of learning rate\n",
    "        learning_rate = learning_rate0 / (1+(episode-1)*learning_rate_decay)        \n",
    "\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        for t in range(episode_length):\n",
    "\n",
    "            system.update_state()\n",
    "            s_current = system.state_to_string()                \n",
    "            p = np.random.uniform()\n",
    "\n",
    "            if p > epsilon:\n",
    "                #DETERMINISTIC ACTION OPTIMAL\n",
    "                s0 = system.state_to_string()\n",
    "                best_action = optimal_policy(s0, Q)\n",
    "                if best_action == None:\n",
    "                    reward = system.random_action(seed = (seed + (t+1)*episode), verbose = verbose)\n",
    "                else:\n",
    "                    reward = system.deterministic_action(best_action)\n",
    "                #print(best_action)\n",
    "            else:\n",
    "                reward = system.random_action(seed = (seed + (t+1)*episode), verbose = verbose)\n",
    "\n",
    "            a_current = system.action_to_string()\n",
    "            sa_current = ''.join([s_current, a_current])\n",
    "\n",
    "            system.update_state()\n",
    "            sa_new = system.state_action_to_string()\n",
    "\n",
    "            if ut.is_key(Q, sa_current) == False:\n",
    "                Q[sa_current] = 0\n",
    "\n",
    "            Q_max = max([Q[key] for key in Q.keys() if key.startswith(sa_new[0:system.state_length])]+[0.0]) \n",
    "\n",
    "            if Q[sa_current] != -np.inf:\n",
    "                Q[sa_current] = ( (1-learning_rate) * Q[sa_current] \n",
    "                                 + learning_rate* (reward + discount_rate * Q_max)\n",
    "                                )\n",
    "                \n",
    "            discounted_reward = discounted_reward + (discount_rate**t) * reward\n",
    "            system.reset_trucks_positions();     \n",
    "            system.reset_trucks_loads();\n",
    "            \n",
    "        rewards_list.append(discounted_reward);\n",
    "        if episode % freq == 0:\n",
    "                time_end = time.time()\n",
    "                print(\". Elapsed time \", round( (time_end-time_start)/60., round_time), \" minuts.\",\n",
    "                      \"epsilon\", round(epsilon,4), \n",
    "                     \"Discounted reward: \", discounted_reward)\n",
    "                \n",
    "                if verbose_info:\n",
    "                    print(\"s, a\", system.s, system.a)\n",
    "                    print(\"ds, da\", system.ds, system.da)\n",
    "        if episode % Q_freq == 0:           \n",
    "                ut.save_obj(Q, simulation_directory +\"/Q-dictionaries/Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")   \n",
    "\n",
    "        if episode % vis_freq == 0:\n",
    "                #Save visualization and rewards\n",
    "                visualization_steps.append(toy_system.visualize());\n",
    "                ut.save_obj(visualization_steps, simulation_directory +\"/vis/vis-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")   \n",
    "                                \n",
    "        if episode % rew_freq == 0:\n",
    "            #rewards_list.append(discounted_reward);\n",
    "                ut.save_obj(rewards_list, simulation_directory +\"/discrewards/discrew-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")\n",
    "                \n",
    "    end_time = round(time.time()-time_start,round_time)        \n",
    "    print(f\"Training finished. Total episodes: {n_episodes}. Elapsed time: {round(end_time/60., round_time)} minutes.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a state, returns the action that has the highest Q-value.\n",
    "\n",
    "def optimal_policy(state, Q, system = toy_system):\n",
    "    \"\"\"\n",
    "    state must be in the string-integers code\n",
    "    \"\"\"\n",
    "    state_keys = [key for key in list(Q) if key.startswith(state)]\n",
    "    if len(state_keys) == 0:\n",
    "        return(None)\n",
    "    \n",
    "    state_q = [Q[state_key] for state_key in state_keys]\n",
    "    \n",
    "    #print(\"state_q \", state_q[1:min(10,len(state_q))])\n",
    "    \n",
    "    max_q = max(state_q)\n",
    "    #print(\"max_q\", max_q)\n",
    "    optimal_key_index = np.where(np.isin(state_q, max_q ))[0][0]\n",
    "    #print(\"optimal_key_index\", optimal_key_index)\n",
    "    optimal_key = state_keys[optimal_key_index]\n",
    "    #print(\"optimal_key\", optimal_key)\n",
    "    optimal_action = optimal_key[system.state_length:]\n",
    "    \n",
    "    return(optimal_action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100. Elapsed time  0.05  minuts. epsilon 0.5025 Discounted reward:  -18000033.008006\n",
      "Episode: 200. Elapsed time  0.12  minuts. epsilon 0.3344 Discounted reward:  -10000015.0270175\n",
      "Episode: 300. Elapsed time  0.2  minuts. epsilon 0.2506 Discounted reward:  -4000027.000789937\n",
      "Episode: 400. Elapsed time  0.29  minuts. epsilon 0.2004 Discounted reward:  -6000004.036557288\n",
      "Episode: 500. Elapsed time  0.4  minuts. epsilon 0.1669 Discounted reward:  -3000005.0267283157\n",
      "Episode: 600. Elapsed time  0.5  minuts. epsilon 0.1431 Discounted reward:  -3000007.004646487\n",
      "Episode: 700. Elapsed time  0.62  minuts. epsilon 0.1252 Discounted reward:  -1000003.0370675306\n",
      "Episode: 800. Elapsed time  0.74  minuts. epsilon 0.1112 Discounted reward:  -3000007.007663468\n",
      "Episode: 900. Elapsed time  0.86  minuts. epsilon 0.1001 Discounted reward:  -5000010.315892206\n",
      "Episode: 1000. Elapsed time  0.98  minuts. epsilon 0.091 Discounted reward:  -4000009.0529886945\n",
      "Episode: 1100. Elapsed time  1.1  minuts. epsilon 0.0834 Discounted reward:  -3000004.1607571184\n",
      "Episode: 1200. Elapsed time  1.23  minuts. epsilon 0.077 Discounted reward:  -3000002.034115862\n",
      "Episode: 1300. Elapsed time  1.35  minuts. epsilon 0.0715 Discounted reward:  -1000001.011346814\n",
      "Episode: 1400. Elapsed time  1.48  minuts. epsilon 0.0667 Discounted reward:  -3000008.009738318\n",
      "Episode: 1500. Elapsed time  1.63  minuts. epsilon 0.0625 Discounted reward:  -3000004.008771435\n",
      "Episode: 1600. Elapsed time  1.76  minuts. epsilon 0.0589 Discounted reward:  -3000007.3435825836\n",
      "Episode: 1700. Elapsed time  1.9  minuts. epsilon 0.0556 Discounted reward:  -4000009.0142422207\n",
      "Episode: 1800. Elapsed time  2.03  minuts. epsilon 0.0527 Discounted reward:  -2000005.0123152758\n",
      "Episode: 1900. Elapsed time  2.17  minuts. epsilon 0.05 Discounted reward:  -2000002.0068309337\n",
      "Episode: 2000. Elapsed time  2.31  minuts. epsilon 0.05 Discounted reward:  -3.0133256281865384\n",
      "Episode: 2100. Elapsed time  2.45  minuts. epsilon 0.05 Discounted reward:  -2000005.041301531\n",
      "Episode: 2200. Elapsed time  2.59  minuts. epsilon 0.05 Discounted reward:  -1.996798623989531\n",
      "Episode: 2300. Elapsed time  2.72  minuts. epsilon 0.05 Discounted reward:  -3.0438762969515984\n",
      "Episode: 2400. Elapsed time  2.86  minuts. epsilon 0.05 Discounted reward:  -3000002.032521923\n",
      "Episode: 2500. Elapsed time  3.0  minuts. epsilon 0.05 Discounted reward:  -0.02332281152084067\n",
      "Episode: 2600. Elapsed time  3.14  minuts. epsilon 0.05 Discounted reward:  -2.0692034610643004\n",
      "Episode: 2700. Elapsed time  3.28  minuts. epsilon 0.05 Discounted reward:  -7.217735090397938\n",
      "Episode: 2800. Elapsed time  3.42  minuts. epsilon 0.05 Discounted reward:  -7.992701193601943\n",
      "Episode: 2900. Elapsed time  3.56  minuts. epsilon 0.05 Discounted reward:  -1000005.010819177\n",
      "Episode: 3000. Elapsed time  3.7  minuts. epsilon 0.05 Discounted reward:  -1.0077631472772164\n",
      "Episode: 3100. Elapsed time  3.85  minuts. epsilon 0.05 Discounted reward:  -1000005.0058473583\n",
      "Episode: 3200. Elapsed time  4.0  minuts. epsilon 0.05 Discounted reward:  -999999.9937356734\n",
      "Episode: 3300. Elapsed time  4.16  minuts. epsilon 0.05 Discounted reward:  -1000000.9987396167\n",
      "Episode: 3349"
     ]
    }
   ],
   "source": [
    "if train_epsilon == True and retrain == False:\n",
    "    episodic_train_Q_epsilon()\n",
    "elif train_epsilon == False and retrain == True:\n",
    "    Q_retrain = ut.load_obj(simulation_directory + \"/Q-dictionaries/Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode_retrain}\")\n",
    "    episodic_train_Q_epsilon(n_episodes = retrain_episodes, Q = Q_retrain)\n",
    "else:\n",
    "    raise Exception(\"Only one of the parameters train_epsilon or retrain parameters can be set to True.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST PARAMETERS AND INITIALIZATION\n",
    "\n",
    "# Initialize system\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system(seed =100000)\n",
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "    \n",
    "Q = ut.load_obj(simulation_directory + \"/Q-dictionaries/Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episodes}\")\n",
    "\n",
    "\n",
    "test_episodes = 5\n",
    "episode_length =30\n",
    "test_freq = 1\n",
    "test_verbose = False\n",
    "\n",
    "test_visualization_steps = []\n",
    "test_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_Q(n_episodes = test_episodes, \n",
    "           episode_length = episode_length,\n",
    "           system = test_toy_system,\n",
    "           visualization_steps = test_visualization_steps, \n",
    "           rewards_list = test_rewards_list,\n",
    "           freq = test_freq,\n",
    "           test_verbose = test_verbose\n",
    "           \n",
    "          ):\n",
    "    \n",
    "    for episode in range(1,n_episodes+1): \n",
    "        print(\"\\rTest episode: {}\".format(episode), end=\"\")\n",
    "        reinitialize_system(system, seed = episode+episodes)\n",
    "\n",
    "        discounted_reward = 0      \n",
    "        \n",
    "        for i in range(1,episode_length+1):\n",
    "            #print(\"state\", test_toy_system.s, test_toy_system.ds)\n",
    "            system.update_state()\n",
    "\n",
    "            #Save visualization steps\n",
    "            if i % freq == 0:\n",
    "                visualization_steps.append(system.visualize());\n",
    "\n",
    "            s0 = system.state_to_string()\n",
    "            best_action = optimal_policy(s0, Q)\n",
    "            #print(\"best_action\", best_action)\n",
    "\n",
    "            if best_action == None:\n",
    "                reward = system.random_action()\n",
    "                if i % freq == 0:\n",
    "                    if test_verbose == True:\n",
    "                        print(\"Episode\", episode, \"t\", i-1, reward, \" Random action is performed. Current state unknown for Q.\")\n",
    "\n",
    "            else:\n",
    "                reward = system.deterministic_action(best_action)\n",
    "                if i % freq == 0:\n",
    "                    if test_verbose == True:\n",
    "                        print(\"Episode\", episode, \"t\",i-1,reward, best_action)\n",
    "\n",
    "            system.reset_trucks_positions();\n",
    "            system.reset_trucks_loads();\n",
    "            \n",
    "            discounted_reward = discounted_reward + (discount_rate**(i-1)) * reward\n",
    "            if reward <= P2_GLOBAL:\n",
    "                print(\"\\rSome tank is in a forbidden level\")\n",
    "\n",
    "        system.reset_trucks_positions();\n",
    "        \n",
    "        #Save rewards\n",
    "        if episode % freq == 0:\n",
    "            rewards_list.append(discounted_reward);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Q()\n",
    "#print(np.mean(test_rewards_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.hist(test_rewards_list, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing test simulation:\n",
    "\n",
    "test_anim = ut.create_system_animation(test_visualization_steps, test_episodes * episode_length,test_freq)\n",
    "HTML(test_anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing train simulation:\n",
    "\n",
    "episode =episodes\n",
    "#simulation_id = 2\n",
    "step = 30\n",
    "discrewards_list = ut.load_obj(simulation_directory+\"/discrewards/discrew-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")\n",
    "\n",
    "discrewards_list2 = [discrewards_list[i] for i in range(0,len(discrewards_list),step)]\n",
    "\n",
    "p = plt.plot([i for i in range(0,len(discrewards_list),step)], \n",
    "             discrewards_list2)\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(simulation_directory+'/discrewards-sim' + f'{simulation_id}' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tank_levels = [test_visualization_steps[i][2] for i in range(len(test_visualization_steps))]\n",
    "tank_levels_array = np.asarray(tank_levels).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCRETE LEVELS\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks = test_toy_system.tanks\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, tanks[i-1].levels):\n",
    "        plt.axhline(y= lvl, xmin=0, xmax=episode_length, hold=None, color = lvl_color, linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE REAL LEVELS (percentages 12h, 36h, ? h)\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks = test_toy_system.tanks\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "\n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, tanks[i-1].level_percentages):\n",
    "        plt.axhline(y= lvl * tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "                    linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    \n",
    "    \n",
    "    percentages = tanks[i-1].level_percentages           \n",
    "    c = percentages[1]\n",
    "    e = percentages[2]          \n",
    "    d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "    plt.axhline(y= d*tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "                linestyle = '-.')\n",
    "\n",
    "    plt.axhline(y= np.mean(tank_levels_array[i-1]), xmin=0, xmax=episode_length, hold=None, \n",
    "                color = \"blue\", linestyle = '-.')\n",
    "  \n",
    "\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "#plt.show()\n",
    "plt.savefig(simulation_directory + '/tank-levels-sim' + f'{simulation_id}' + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
