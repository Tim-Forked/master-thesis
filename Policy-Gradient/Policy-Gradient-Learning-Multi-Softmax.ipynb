{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsalgador/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The algorithm is tested on the PDSystemEnv  gym task \n",
    "and developed with Tensorflow\n",
    "\n",
    "Author: Daniel Salgado Rojo\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_pdsystem\n",
    "from gym import wrappers\n",
    "#import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "#from ddpg.replay_buffer import ReplayBuffer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "import gym_pdsystem.utils.functions as fnc\n",
    "\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "\n",
    "#TO OMMIT WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "simulations_directory = './simulations'\n",
    "if not os.path.exists(simulations_directory):\n",
    "    os.makedirs(simulations_directory)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_id = 39\n",
    "\n",
    "# Create directories for the simulations' outputs\n",
    "\n",
    "simulation_directory = './simulations/simulation{}'.format(simulation_id)\n",
    "rewards_file = simulation_directory+'/discrewards'+ '/rewards-sim{}.csv'.format(simulation_id)\n",
    "states_file = simulation_directory+'/vis'+ '/states-sim{}.csv'.format(simulation_id) # for testing\n",
    "models_dir = simulation_directory + '/NN-models'\n",
    "\n",
    "if not os.path.exists(simulation_directory):\n",
    "    os.makedirs(simulation_directory)\n",
    "    os.makedirs(models_dir)\n",
    "    os.makedirs(simulation_directory + '/discrewards')\n",
    "    os.makedirs(simulation_directory + '/vis')\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"The simulation id you tried to use has been already used before. Try to change it to a new one.\")\n",
    "\n",
    "# Output headers to file\n",
    "with open(rewards_file,'w') as f:\n",
    "     np.savetxt(f, [[\"iteration\", \"discreward\"]], fmt=\"%s\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Example n=5, k = 2\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 0.])\n",
    "DISCRETE = True\n",
    "\n",
    "n = len(TANK_MAX_LOADS)\n",
    "k = len(TRUCK_MAX_LOADS)\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "# ###########################################################\n",
    "# # Example n=9, k = 3\n",
    "\n",
    "# TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.])\n",
    "# LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "#                                                 [0.02, 0.31, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.05, 0.16, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85],\n",
    "#                                                 [0.08, 0.26, 0.9],\n",
    "#                                                 [0.02, 0.31, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.05, 0.16, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85]\n",
    "#                                                    ])\n",
    "\n",
    "# TRUCK_MAX_LOADS = np.array([70.,130.,250.])\n",
    "\n",
    "# GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156., 0.])\n",
    "# DISCRETE = True\n",
    "# ############################################################\n",
    "\n",
    "\n",
    "\n",
    "# ###########################################################\n",
    "# \t# Example n=11, k = 3\n",
    "\n",
    "# \tTANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.,600.,900.])\n",
    "# \tLEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "# \t                                                [0.02, 0.31, 0.9],\n",
    "# \t                                                [0.01, 0.03, 0.9],\n",
    "# \t                                                [0.05, 0.16, 0.9],\n",
    "# \t                                                [0.07, 0.14, 0.85],\n",
    "# \t                                                [0.08, 0.26, 0.9],\n",
    "# \t                                                [0.02, 0.31, 0.9],\n",
    "# \t                                                [0.01, 0.03, 0.9],\n",
    "# \t                                                [0.05, 0.16, 0.9],\n",
    "# \t                                                [0.07, 0.14, 0.85],\n",
    "# \t                                                [0.01, 0.03, 0.9],\n",
    "# \t                                                [0.01, 0.03, 0.9]\n",
    "\n",
    "# \t                                                   ])\n",
    "\n",
    "# \tTRUCK_MAX_LOADS = np.array([70.,130.,250.])\n",
    "\n",
    "# \tGRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156.,150.,150., 0.])\n",
    "# \tDISCRETE = True\n",
    "# \t############################################################\n",
    "    \n",
    "    \n",
    "# ###########################################################\n",
    "# # Example n=12, k = 3\n",
    "\n",
    "# TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.,600.,900.,700.])\n",
    "# LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "#                                                 [0.02, 0.31, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.05, 0.16, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85],\n",
    "#                                                 [0.08, 0.26, 0.9],\n",
    "#                                                 [0.02, 0.31, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.05, 0.16, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85]\n",
    "\n",
    "\n",
    "#                                                    ])\n",
    "\n",
    "# TRUCK_MAX_LOADS = np.array([70.,130.,250.])\n",
    "\n",
    "# GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156.,150.,150.,150., 0.])\n",
    "# DISCRETE = True\n",
    "# ############################################################\n",
    "\n",
    "# ###########################################################\n",
    "# # Example n=12, k = 4\n",
    "\n",
    "# TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.,600.,900.,700.])\n",
    "# LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "#                                                 [0.02, 0.31, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.05, 0.16, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85],\n",
    "#                                                 [0.08, 0.26, 0.9],\n",
    "#                                                 [0.02, 0.31, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.05, 0.16, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.01, 0.03, 0.9],\n",
    "#                                                 [0.07, 0.14, 0.85]\n",
    "\n",
    "\n",
    "#                                                    ])\n",
    "\n",
    "# TRUCK_MAX_LOADS = np.array([70.,130.,250., 200.])\n",
    "\n",
    "# GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156.,150.,150.,150., 0.])\n",
    "# DISCRETE = True\n",
    "# ############################################################\n",
    "\n",
    "\n",
    "env = gym.make(\"PDSystemEnv-v0\")\n",
    "episode_length = 30\n",
    "env._max_episode_steps = episode_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_action(int_action: int, env):\n",
    "    \"\"\"\n",
    "    So far assumed k = 2, 3 or 4:\n",
    "    \n",
    "    Converts an integer between 0 and env.action_space.shape[1]**env.action_space.shape[0]\n",
    "    which is (n+1)^k where n is the number of tanks and k the number of trucks.\n",
    "    \n",
    "    return vect_action: a k-dimensional vector with components in the range 0,...n. \n",
    "    For k = 2, vect_action = [i,j] is the action of truck 1 going to tank i and truck 2 going to tank j.\n",
    "    (i, j = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is i*(n+1) + j\n",
    "    \n",
    "    For k = 3, vect_action = [i,j,l] is the action of truck 1 going to tank i, truck 2 going to tank j,\n",
    "    and truck 3 going to tank l.\n",
    "    (i, j, l = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is (i*(n+1) + j)*(n+1) + l\n",
    "    \n",
    "    For k = 4, vect_action = [i,j,l,m] is the action of truck 1 going to tank i, truck 2 going to tank j,\n",
    "    truck 3 going to tank l and truck 4 going to tank m.\n",
    "    (i, j, l, m = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is ((i*(n+1) + j)*(n+1) + l)*(n+1) + m\n",
    "\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    n_actions = nplus1**k\n",
    "    \n",
    "    if k == 2:\n",
    "        j = int_action % nplus1\n",
    "        i = int((int_action-j)/nplus1)\n",
    "        vect_action = np.array([i,j])\n",
    "      \n",
    "    elif k == 3:\n",
    "        l = int_action % nplus1\n",
    "        ij = int( (int_action - l)/nplus1 ) \n",
    "        j = ij % nplus1\n",
    "        i = int((ij-j)/nplus1)\n",
    "        vect_action = np.array([i,j,l])\n",
    "        \n",
    "    elif k == 4:\n",
    "        m = int_action % nplus1\n",
    "        ml = int((int_action - m)/nplus1)\n",
    "        l = ml % nplus1\n",
    "        ij = int((ml-l)/nplus1) \n",
    "        j = ij % nplus1\n",
    "        i = int((ij-j)/nplus1)\n",
    "        vect_action = np.array([i,j,l,m])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2, 3 or 4\")\n",
    "    return vect_action\n",
    "\n",
    "def action_to_int(vect_action: np.array, env):\n",
    "    \"\"\"\n",
    "    Assumed k = 2,3 or 4, so vect_action has 2, 3 or components respectively.\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    if k == 2:\n",
    "        int_action = vect_action[0] * nplus1 + vect_action[1]\n",
    "    elif k == 3:\n",
    "        int_action = (vect_action[0] * nplus1 + vect_action[1])*nplus1 + vect_action[2]\n",
    "    elif k == 4:\n",
    "        int_action = ((vect_action[0] * nplus1 + vect_action[1])*nplus1 + vect_action[2])*nplus1 + vect_action[3] \n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2, 3 or 4\")\n",
    "\n",
    "    return int_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "[5 5]\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "int_action = 35\n",
    "print(int_action)\n",
    "vect_action = int_to_action(int_action,env)\n",
    "print(vect_action)\n",
    "int_action = action_to_int(vect_action,env)\n",
    "print(int_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ageron/handson-ml \n",
    "\"\"\"\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def normalize_rewards(all_discounted_rewards): #, discount_rate):\n",
    "    #all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Tensor(\"dnn/concat_1:0\", shape=(?, 12), dtype=float32)\n",
      "(1,)\n",
      "[(<tf.Tensor 'train/gradients/dnn/dense/MatMul_grad/tuple/control_dependency_1:0' shape=(5, 100) dtype=float32>, <tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/dense/BiasAdd_grad/tuple/control_dependency_1:0' shape=(100,) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/dense_1/MatMul_grad/tuple/control_dependency_1:0' shape=(100, 50) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(100, 50) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/dense_1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(50,) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(50,) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/fully_connected/MatMul_grad/tuple/control_dependency_1:0' shape=(50, 6) dtype=float32>, <tf.Variable 'fully_connected/weights:0' shape=(50, 6) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0' shape=(6,) dtype=float32>, <tf.Variable 'fully_connected/biases:0' shape=(6,) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0' shape=(50, 6) dtype=float32>, <tf.Variable 'fully_connected_1/weights:0' shape=(50, 6) dtype=float32_ref>), (<tf.Tensor 'train/gradients/dnn/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(6,) dtype=float32>, <tf.Variable 'fully_connected_1/biases:0' shape=(6,) dtype=float32_ref>)]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "# TensorBoard summary directories\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)   \n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "learning_rate = 0.01 #0.01\n",
    "\n",
    "hidden1_neurons = 100 #100\n",
    "hidden2_neurons = 50 #50\n",
    "\n",
    "batch_normalization = True  \n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "print(env.action_space.shape[1])\n",
    "with tf.name_scope(\"dnn\"):\n",
    "        \n",
    "            # 1. Parameters to determine the NN architecture\n",
    "\n",
    "            n_inputs = env.observation_space.shape[1]\n",
    "            n_hidden1 = hidden1_neurons; activation1 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_hidden2 = hidden2_neurons; activation2 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_outputs = env.action_space.shape[1]*env.action_space.shape[0]\n",
    "            \n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            # 2. Build the Neural Network\n",
    "#             training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "            \n",
    "#             my_batch_norm_layer = partial(\n",
    "#                                             tf.layers.batch_normalization,\n",
    "#                                             training=training,\n",
    "#                                             momentum=batch_norm_momentum)\n",
    "\n",
    "#             my_dense_layer = partial(\n",
    "#                                         tf.layers.dense,\n",
    "#                                         kernel_initializer=he_init)\n",
    "\n",
    "\n",
    "            X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "            \n",
    "            \n",
    "#             hidden1 = my_dense_layer(X, n_hidden1, name = \"hidden1\")\n",
    "#             bn1 = activation1(my_batch_norm_layer(hidden1))\n",
    "#             hidden2 = my_dense_layer(bn1, n_hidden2, name = \"hidden2\")\n",
    "#             bn2 = activation2(my_batch_norm_layer(hidden2))\n",
    "            \n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, activation = activation1,\n",
    "                                     kernel_initializer = he_init)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = activation2,\n",
    "                                     kernel_initializer = he_init)\n",
    "            \n",
    "            logs = []\n",
    "            for i in range(k): \n",
    "                log = tf.contrib.slim.fully_connected(hidden2,n+1,\n",
    "                                             activation_fn=None)                      \n",
    "                logs.append(log)\n",
    "                \n",
    "            logits = tf.concat(logs, axis=1)\n",
    "            \n",
    "#             logits = tf.layers.dense(hidden2, n_outputs, name = \"logits\")#,kernel_initializer = initializer)\n",
    "            \n",
    "            \n",
    "            #\n",
    "#             outputs2 = tf.nn.softmax(logits)\n",
    "#             print(outputs2)\n",
    "            outs = []\n",
    "       \n",
    "            for i in range(k): \n",
    "                truck = tf.nn.softmax(logs[i])\n",
    "                      \n",
    "                outs.append(truck)\n",
    "\n",
    "            outputs = tf.concat(outs, axis=1)\n",
    "            print(outputs)\n",
    "                                          \n",
    "            \n",
    "with tf.name_scope(\"action\"):\n",
    "            # 3. Select a random action (where to go) based on the estimated probabilities\n",
    "            actions = []\n",
    "            ys = []\n",
    "            for i in range(k): \n",
    "                action = tf.multinomial(tf.log(outs[i]), num_samples = 1)\n",
    "                actions.append(action) \n",
    "                ys.append(tf.reshape(action, [1]))\n",
    "#              action = tf.multinomial(tf.log(outputs2), num_samples = 1) \n",
    "#              y = tf.reshape(action, [1])\n",
    "            #action = tf.concat(actions, axis = 1)\n",
    "            #print(action)\n",
    "            #test_action = tf.argmax(outputs, axis = 1)\n",
    "            \n",
    "            \n",
    "with tf.name_scope(\"loss\"):\n",
    "            xentropies = []\n",
    "            for i in range(k):         \n",
    "                xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = ys[i],logits = logs[i])\n",
    "#                 print(logs[i].shape)\n",
    "                xentropies.append(xentropy)\n",
    "# #             xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y,\n",
    "# #                                                                       logits = logits)\n",
    "#             #xentropy = tf.concat(xentropies, axis = 1)\n",
    "#             print(len(xentropies) )\n",
    "            xentropy = sum(xentropies)\n",
    "            print(xentropy.shape)\n",
    "#             xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = logits)\n",
    "#             loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "            \n",
    "tf.summary.scalar('average_cross_entropy', loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "            # Optimization Op\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #optimize = optimizer.minimize(loss)\n",
    "            \n",
    "            grads_and_vars = optimizer.compute_gradients(xentropy)\n",
    "            print(grads_and_vars)\n",
    "            gradients = [grad for grad, variable in grads_and_vars]\n",
    "            gradient_placeholders = []\n",
    "            grads_and_vars_feed = []\n",
    "            for grad, variable in grads_and_vars:\n",
    "                gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "                gradient_placeholders.append(gradient_placeholder)\n",
    "                grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "            training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "                        \n",
    "# # with tf.name_scope(\"eval\"):\n",
    "# #             correct = tf.nn.in_top_k(logits, y, 1)\n",
    "# #             accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "# # tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "#Extra summary scalars\n",
    "# avg_disc_reward_per_game = tf.placeholder(tf.float32, name = \"avg_disc_reward\")\n",
    "# reward_placeholder = tf.placeholder(tf.float32, name = \"reward_placeholder\")\n",
    "# avg_disc_reward_per_game = tf.Variable(0.0,dtype = tf.float32, name = \"avg_disc_reward\")\n",
    "\n",
    "# tf.summary.scalar(\"Average_Discounted_Reward_per_game\", avg_disc_reward_per_game)\n",
    "                                       \n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Average reward per game:  -2.701522 , Elapsed time  0.01  minutes.\n",
      "Saved Model with average discounted rewards per game  -2.7015219690677696\n",
      "Iteration: 20Saved Model with average discounted rewards per game  -1.4252171608830493\n",
      "Iteration: 27"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c8bf2f2eea5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0maction_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mvect_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#HERE WE CONVERT FROM INTEGER TO ACTION's Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# File names for the model\n",
    "\n",
    "model_file = models_dir + \"/pdenv_policy_net_pg.ckpt\"#.format(learning_rate)\n",
    "best_model_file = models_dir + \"/pdenv_best_policy_net_pg.ckpt\"#.format(learning_rate)\n",
    "\n",
    "graph_file =  '{}.meta'.format(model_file)\n",
    "\n",
    "\n",
    "# Simulation / Training parameters\n",
    "n_games_per_update = 10\n",
    "n_max_steps = episode_length\n",
    "n_iterations = 500 #2*10**4\n",
    "save_iterations = 100 # USEFUL WHEN USING EARLY STOPPING\n",
    "discount_rate = 0.9\n",
    "\n",
    "\n",
    "info_freq = 100 #100\n",
    "round_time = 2\n",
    "round_reward = 6\n",
    "#avg_rewards_list = []\n",
    "summary_freq = 20#int(info_freq/2), 10\n",
    "\n",
    "# Early stopping\n",
    "max_checks_without_progress = np.inf # 10,  one for each \"info_freq\" (in terms of iterations it would be\n",
    "                                                            #max_checks_without_progress *  info_freq)\n",
    "checks_without_progress = 0\n",
    "best_reward = -np.infty\n",
    "\n",
    "\n",
    "retrain = False\n",
    "###########################\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #train_writer = tf.summary.FileWriter(logdir + '/pgtrain', sess.graph)\n",
    "    if retrain:\n",
    "        saver.restore(sess, model_file)\n",
    "        \n",
    "    summary2 = tf.Summary()\n",
    "    if not retrain:\n",
    "        init.run()\n",
    "    time_start = time.time()\n",
    "\n",
    "    for iteration in range(n_iterations+1):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)}) \n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                obs, reward, done, info = env.step(vect_action)\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)                \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        \n",
    "        all_disc_rewards =  [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "        all_disc_norm_rewards = normalize_rewards(all_disc_rewards)     \n",
    "\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_disc_norm_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "         \n",
    "       \n",
    "        #tf.summary.scalar('avg_rewards', avg_rewards)\n",
    "        #train_writer.add_summary(avg_rewards, iteration)\n",
    "\n",
    "        # Summary and info printings     ###################################################################\n",
    "        if iteration % info_freq == 0 or iteration % summary_freq == 0:  \n",
    "            \n",
    "            #avg_rewards =np.array([np.mean(np.array(all_rewards)) / n_games_per_update])\n",
    "            avg_rewards =np.mean(np.array(all_disc_rewards)) / n_games_per_update\n",
    "#             print(avg_rewards)\n",
    "            #avg_rewards_list.append(avg_rewards)\n",
    "            if iteration % summary_freq == 0:\n",
    "                summary2.value.add(tag='average_reward_per_game', simple_value = avg_rewards)\n",
    "                file_writer.add_summary(summary2, iteration)\n",
    "\n",
    "                feed_dict[X] = obs.reshape(1, n_inputs)\n",
    "                summary = sess.run(merged, feed_dict= feed_dict)\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                  # Disc rewards, etc ####\n",
    "       \n",
    "                with open(rewards_file,'ab') as f:\n",
    "                            np.savetxt(f, [np.array([iteration,  avg_rewards])], fmt=(\"%d\",'%.10f'), delimiter=',') \n",
    "\n",
    "            if iteration % info_freq == 0:\n",
    "                time_end = time.time()\n",
    "                print(\". Average reward per game: \",  round(avg_rewards, round_reward), \n",
    "                      \", Elapsed time \", round( (time_end-time_start)/60., round_time), \" minutes.\") \n",
    "                \n",
    "                \n",
    "        ###########################################################################3\n",
    "        \n",
    "      \n",
    "                \n",
    "        if avg_rewards > best_reward:\n",
    "                    saver.save(sess, best_model_file)\n",
    "                    print(\"Saved Model with average discounted rewards per game \", avg_rewards)\n",
    "                    best_reward = avg_rewards\n",
    "                    checks_without_progress = 0\n",
    "        else:\n",
    "                    checks_without_progress += 1\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "        ####################################################################################################\n",
    "        if iteration % save_iterations == 0:\n",
    "                 saver.save(sess, model_file)\n",
    "        \n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "    \n",
    "    #saver.save(sess, model_file + \"final\")\n",
    "    file_writer.flush()\n",
    "    file_writer.close()\n",
    "                 \n",
    "    time_end = time.time()\n",
    "    print(\"Simulation finished. Elapsed time \", round( (time_end-time_start)/60., round_time), \" minutes.\") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing train simulation:\n",
    "\n",
    "#episode =episodes\n",
    "#simulation_id = 2\n",
    "#step = 30\n",
    "\n",
    "discrewards = pd.read_table(rewards_file, sep = \",\")\n",
    "\n",
    "indices = [i for i in range(0,n_iterations,1)]\n",
    "\n",
    "plt.plot(discrewards['iteration'][indices],discrewards['discreward'][indices], label = \"Discounted rewards\")\n",
    "plt.title(\"Discounted rewards per episode during training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Discounted reward\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 45\n",
    "np.random.seed(seed)\n",
    "\n",
    "frames = []\n",
    "n_episodes = 1\n",
    "\n",
    "system = PDSystemEnv()\n",
    "#model_file = \"pdenv_policy_net_pg.ckpt\"\n",
    "test_model_file = model_file\n",
    "\n",
    "with open(states_file,'w') as f:\n",
    "         np.savetxt(f, [[\"step\", \"stock0\", \"stock1\", \"stock2\",\"stock3\", \"stock4\"]], fmt=\"%s\", delimiter=',') \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, test_model_file)\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            for step in range(episode_length):\n",
    "                system.state = state\n",
    "                with open(states_file,'ab') as f:\n",
    "                    np.savetxt(f, [np.array([(episode)*(episode_length)+step]+state.tolist())], fmt=(\"%d\",'%.3f','%.3f','%.3f','%.3f','%.3f'), delimiter=',') \n",
    "                img = system.visualize()\n",
    "                frames.append(img)\n",
    "\n",
    "                action_val = action.eval(feed_dict={X: state.reshape(1, n_inputs)})\n",
    "                #print(action_val)\n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                #print(vect_action)\n",
    "                state, reward, done, info = env.step(vect_action)\n",
    "                #print(state)\n",
    "                #print(action_val[0],emptiest_tank_policy(state, system))\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anim = ut.create_system_animation(frames, n_episodes * episode_length)\n",
    "plt.close()\n",
    "\n",
    "HTML(test_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_simulation(states_file,\n",
    "                        test_toy_system,\n",
    "                        simulation_id,\n",
    "                        simulation_directory):\n",
    "    test_states = pd.read_table(states_file, sep = \",\")\n",
    "    test_states.head()\n",
    "\n",
    "\n",
    "    cmap = plt.get_cmap('gnuplot')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "    lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "    #tanks = test_toy_system.tanks\n",
    "    tanks_max_load = system.tank_max_loads\n",
    "    level_percentages = system.load_level_percentages\n",
    "\n",
    "    for i, color in enumerate(colors, start=1):    \n",
    "        plt.subplot(2,3,i)\n",
    "        states = test_states['stock{}'.format(i-1)]\n",
    "        plt.plot(test_states['step'],states, label = \"Shop {}\".format(i-1), color = \"Black\")\n",
    "        plt.title(\"Shop {}\".format(i-1))\n",
    "\n",
    "        plt.axhline(y= tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "        for lvl_color, lvl in zip(lvl_colors, level_percentages[i-1]):\n",
    "            plt.axhline(y= lvl * tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "                        linestyle = '--')\n",
    "        plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "\n",
    "\n",
    "        percentages = level_percentages[i-1]           \n",
    "        c = percentages[1]\n",
    "        e = percentages[2]          \n",
    "        d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "        plt.axhline(y= d*tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "                    linestyle = '-.')\n",
    "\n",
    "        plt.axhline(y= np.mean(states), xmin=0, xmax=episode_length, hold=None, \n",
    "                    color = \"blue\", linestyle = '-.')\n",
    "        plt.xticks(range(0,episode_length*n_episodes+1,5*n_episodes\n",
    "                        ))\n",
    "\n",
    "\n",
    "    plt.subplot(2,3,6)\n",
    "    plt.title(\"Legend\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    #plt.legend(bbox_to_anchor=(0.5 ,0.8), loc=2, borderaxespad=0.)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "\n",
    "    patch1 = mpatches.Patch(color='red', label='Max/Min capacities', linestyle = '--', fill = False, linewidth = 2)\n",
    "    patch2 = mpatches.Patch(color='orange', label='Max/Min levels', linestyle = '--', fill = False, linewidth = 2)\n",
    "    patch3 = mpatches.Patch(color='green', label='Danger level', linestyle = '--', fill = False, linewidth = 2)\n",
    "    patch4 = mpatches.Patch(color='lawngreen', label='Max reward level (M)', linestyle = '-.', fill = False, linewidth = 2)\n",
    "    patch5 = mpatches.Patch(color='blue', label='Observed mean level', linestyle = '-.', fill = False, linewidth = 2)\n",
    "    patch6 = mpatches.Patch(color='black', label='Current stock level', linestyle = '-', fill = False, linewidth = 2)\n",
    "\n",
    "\n",
    "    plt.legend(handles=[patch1, patch2, patch3, patch4, patch5, patch6], loc = 'center')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(simulation_directory + '/tank-levels-sim' + '{}'.format(simulation_id) + '.pdf')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_simulation(states_file,\n",
    "                        system,\n",
    "                        simulation_id,\n",
    "                        simulation_directory)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.close()\n",
    "\n",
    "# #THE REAL LEVELS (percentages 12h, 36h, ? h)\n",
    "# tank_levels = [frames[i][2] for i in range(len(frames))]\n",
    "# tank_levels_array = np.asarray(tank_levels).transpose()\n",
    "\n",
    "# n = system.n\n",
    "\n",
    "# cmap = plt.get_cmap('gnuplot')\n",
    "# colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "# lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "# tanks_max_load = system.tank_max_loads\n",
    "# level_percentages = system.load_level_percentages\n",
    "\n",
    "# for i, color in enumerate(colors, start=1):\n",
    "#     plt.subplot(4,3, i)    \n",
    "\n",
    "#     plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "#     plt.axhline(y= tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "#     for lvl_color, lvl in zip(lvl_colors, level_percentages[i-1]):\n",
    "#         plt.axhline(y= lvl * tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "#                     linestyle = '--')\n",
    "#     plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    \n",
    "    \n",
    "#     percentages = level_percentages[i-1]           \n",
    "#     c = percentages[1]\n",
    "#     e = percentages[2]          \n",
    "#     d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "#     plt.axhline(y= d*tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "#                 linestyle = '-.')\n",
    "\n",
    "#     plt.axhline(y= np.mean(tank_levels_array[i-1]), xmin=0, xmax=episode_length, hold=None, \n",
    "#                 color = \"blue\", linestyle = '-.')\n",
    "    \n",
    "#     plt.legend(loc='best')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_action(3,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 2 or 1 == 0:\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Iteration: 0. Average reward per game:  -0.254854 , Elapsed time  0.01  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.2548537782714528\n",
    "Iteration: 20Saved Model with average discounted rewards per game  -0.16291184851218668\n",
    "Iteration: 40Saved Model with average discounted rewards per game  -0.14633813267939672\n",
    "Iteration: 60Saved Model with average discounted rewards per game  -0.10346410949189291\n",
    "Iteration: 80Saved Model with average discounted rewards per game  -0.10308086893828072\n",
    "Iteration: 100. Average reward per game:  -0.085253 , Elapsed time  0.69  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.08525252061450035\n",
    "Iteration: 120Saved Model with average discounted rewards per game  -0.06821313880371498\n",
    "Iteration: 140Saved Model with average discounted rewards per game  -0.05517269859764351\n",
    "Iteration: 160Saved Model with average discounted rewards per game  -0.04510953993345579\n",
    "Iteration: 200. Average reward per game:  -0.040489 , Elapsed time  1.36  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.04048864031629608\n",
    "Iteration: 220Saved Model with average discounted rewards per game  -0.040015319411656876\n",
    "Iteration: 240Saved Model with average discounted rewards per game  -0.03739446165921686\n",
    "Iteration: 280Saved Model with average discounted rewards per game  -0.03605199926771465\n",
    "Iteration: 300. Average reward per game:  -0.036504 , Elapsed time  2.01  minutes.\n",
    "Iteration: 320Saved Model with average discounted rewards per game  -0.0360392621783427\n",
    "Iteration: 360Saved Model with average discounted rewards per game  -0.030352123544106047\n",
    "Iteration: 400. Average reward per game:  -0.034682 , Elapsed time  2.68  minutes.\n",
    "Iteration: 460Saved Model with average discounted rewards per game  -0.030088698243884354\n",
    "Iteration: 500. Average reward per game:  -0.030383 , Elapsed time  3.44  minutes.\n",
    "Iteration: 520Saved Model with average discounted rewards per game  -0.025380100201183597\n",
    "Iteration: 600. Average reward per game:  -0.030441 , Elapsed time  4.23  minutes.\n",
    "Iteration: 700. Average reward per game:  -0.030481 , Elapsed time  4.99  minutes.\n",
    "Iteration: 800. Average reward per game:  -0.023089 , Elapsed time  5.76  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.02308906699516057\n",
    "Iteration: 840Saved Model with average discounted rewards per game  -0.022190996227687332\n",
    "Iteration: 900. Average reward per game:  -0.027866 , Elapsed time  6.5  minutes.\n",
    "Iteration: 1000. Average reward per game:  -0.024696 , Elapsed time  7.25  minutes.\n",
    "Iteration: 1100. Average reward per game:  -0.020024 , Elapsed time  7.99  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.02002371823705227\n",
    "Iteration: 1200. Average reward per game:  -0.018354 , Elapsed time  8.86  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.0183537952370543\n",
    "Iteration: 1300. Average reward per game:  -0.028454 , Elapsed time  9.75  minutes.\n",
    "Iteration: 1400. Average reward per game:  -0.021402 , Elapsed time  10.52  minutes.\n",
    "Iteration: 1460Saved Model with average discounted rewards per game  -0.015239182867567817\n",
    "Iteration: 1500. Average reward per game:  -0.009011 , Elapsed time  11.29  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.009011009583484395\n",
    "Iteration: 1600. Average reward per game:  -0.018 , Elapsed time  12.2  minutes.\n",
    "Iteration: 1700. Average reward per game:  -0.010671 , Elapsed time  12.88  minutes.\n",
    "Iteration: 1800. Average reward per game:  -0.015497 , Elapsed time  13.64  minutes.\n",
    "Iteration: 1900. Average reward per game:  -0.013837 , Elapsed time  14.41  minutes.\n",
    "Iteration: 2000. Average reward per game:  -0.02122 , Elapsed time  15.07  minutes.\n",
    "Iteration: 2100. Average reward per game:  -0.014356 , Elapsed time  15.73  minutes.\n",
    "Iteration: 2200. Average reward per game:  -0.015155 , Elapsed time  16.45  minutes.\n",
    "Iteration: 2300. Average reward per game:  -0.019112 , Elapsed time  17.15  minutes.\n",
    "Iteration: 2400. Average reward per game:  -0.024544 , Elapsed time  17.86  minutes.\n",
    "Iteration: 2500. Average reward per game:  -0.023874 , Elapsed time  18.6  minutes.\n",
    "Iteration: 2600. Average reward per game:  -0.021726 , Elapsed time  19.44  minutes.\n",
    "Iteration: 2700. Average reward per game:  -0.022568 , Elapsed time  20.07  minutes.\n",
    "Iteration: 2800. Average reward per game:  -0.021072 , Elapsed time  20.69  minutes.\n",
    "Iteration: 2900. Average reward per game:  -0.017937 , Elapsed time  21.32  minutes.\n",
    "Iteration: 3000. Average reward per game:  -0.017959 , Elapsed time  21.95  minutes.\n",
    "Iteration: 3100. Average reward per game:  -0.020895 , Elapsed time  22.61  minutes.\n",
    "Iteration: 3200. Average reward per game:  -0.01278 , Elapsed time  23.29  minutes.\n",
    "Iteration: 3300. Average reward per game:  -0.01506 , Elapsed time  23.96  minutes.\n",
    "Iteration: 3400. Average reward per game:  -0.020578 , Elapsed time  24.63  minutes.\n",
    "Iteration: 3500. Average reward per game:  -0.024152 , Elapsed time  25.31  minutes.\n",
    "Iteration: 3600. Average reward per game:  -0.012712 , Elapsed time  25.99  minutes.\n",
    "Iteration: 3700. Average reward per game:  -0.011355 , Elapsed time  26.66  minutes.\n",
    "Iteration: 3800. Average reward per game:  -0.024316 , Elapsed time  27.35  minutes.\n",
    "Iteration: 3900. Average reward per game:  -0.011415 , Elapsed time  27.99  minutes.\n",
    "Iteration: 4000. Average reward per game:  -0.011069 , Elapsed time  28.62  minutes.\n",
    "Iteration: 4080Saved Model with average discounted rewards per game  -0.00650472440878191\n",
    "Iteration: 4100. Average reward per game:  -0.008461 , Elapsed time  29.27  minutes.\n",
    "Iteration: 4200. Average reward per game:  -0.02039 , Elapsed time  29.94  minutes.\n",
    "Iteration: 4300. Average reward per game:  -0.021163 , Elapsed time  30.58  minutes.\n",
    "Iteration: 4400. Average reward per game:  -0.016984 , Elapsed time  31.21  minutes.\n",
    "Iteration: 4500. Average reward per game:  -0.010428 , Elapsed time  31.92  minutes.\n",
    "Iteration: 4600. Average reward per game:  -0.007008 , Elapsed time  32.56  minutes.\n",
    "Iteration: 4700. Average reward per game:  -0.01501 , Elapsed time  33.2  minutes.\n",
    "Iteration: 4800. Average reward per game:  -0.009047 , Elapsed time  33.9  minutes.\n",
    "Iteration: 4900. Average reward per game:  -0.010756 , Elapsed time  34.55  minutes.\n",
    "Iteration: 5000. Average reward per game:  -0.013652 , Elapsed time  35.18  minutes.\n",
    "Iteration: 5100. Average reward per game:  -0.009747 , Elapsed time  35.85  minutes.\n",
    "Iteration: 5200. Average reward per game:  -0.014864 , Elapsed time  36.53  minutes.\n",
    "Iteration: 5300. Average reward per game:  -0.009685 , Elapsed time  37.21  minutes.\n",
    "Iteration: 5400. Average reward per game:  -0.021582 , Elapsed time  37.99  minutes.\n",
    "Iteration: 5500. Average reward per game:  -0.016633 , Elapsed time  38.76  minutes.\n",
    "Iteration: 5600. Average reward per game:  -0.031076 , Elapsed time  39.58  minutes.\n",
    "Iteration: 5700. Average reward per game:  -0.012058 , Elapsed time  40.33  minutes.\n",
    "Iteration: 5800. Average reward per game:  -0.016601 , Elapsed time  41.28  minutes.\n",
    "Iteration: 5900. Average reward per game:  -0.015024 , Elapsed time  42.01  minutes.\n",
    "Iteration: 6000. Average reward per game:  -0.016798 , Elapsed time  42.94  minutes.\n",
    "Iteration: 6100. Average reward per game:  -0.01426 , Elapsed time  43.77  minutes.\n",
    "Iteration: 6200. Average reward per game:  -0.018939 , Elapsed time  44.55  minutes.\n",
    "Iteration: 6300. Average reward per game:  -0.023794 , Elapsed time  45.35  minutes.\n",
    "Iteration: 6400. Average reward per game:  -0.014924 , Elapsed time  46.14  minutes.\n",
    "Iteration: 6500. Average reward per game:  -0.010411 , Elapsed time  47.05  minutes.\n",
    "Iteration: 6600. Average reward per game:  -0.00949 , Elapsed time  47.88  minutes.\n",
    "Iteration: 6700. Average reward per game:  -0.015505 , Elapsed time  48.58  minutes.\n",
    "Iteration: 6800. Average reward per game:  -0.023028 , Elapsed time  49.22  minutes.\n",
    "Iteration: 6900. Average reward per game:  -0.013006 , Elapsed time  49.9  minutes.\n",
    "Iteration: 7000. Average reward per game:  -0.019925 , Elapsed time  50.57  minutes.\n",
    "Iteration: 7100. Average reward per game:  -0.016147 , Elapsed time  51.19  minutes.\n",
    "Iteration: 7200. Average reward per game:  -0.013787 , Elapsed time  51.86  minutes.\n",
    "Iteration: 7300. Average reward per game:  -0.017775 , Elapsed time  52.53  minutes.\n",
    "Iteration: 7400. Average reward per game:  -0.020805 , Elapsed time  53.19  minutes.\n",
    "Iteration: 7500. Average reward per game:  -0.009113 , Elapsed time  53.85  minutes.\n",
    "Iteration: 7600. Average reward per game:  -0.022694 , Elapsed time  54.53  minutes.\n",
    "Iteration: 7700. Average reward per game:  -0.013156 , Elapsed time  55.16  minutes.\n",
    "Iteration: 7800. Average reward per game:  -0.012889 , Elapsed time  55.83  minutes.\n",
    "Iteration: 7900. Average reward per game:  -0.007344 , Elapsed time  56.49  minutes.\n",
    "Iteration: 8000. Average reward per game:  -0.008029 , Elapsed time  57.16  minutes.\n",
    "Iteration: 8100. Average reward per game:  -0.010029 , Elapsed time  57.82  minutes.\n",
    "Iteration: 8200. Average reward per game:  -0.02825 , Elapsed time  58.55  minutes.\n",
    "Iteration: 8300. Average reward per game:  -0.011554 , Elapsed time  59.4  minutes.\n",
    "Iteration: 8400. Average reward per game:  -0.011721 , Elapsed time  60.09  minutes.\n",
    "Iteration: 8500. Average reward per game:  -0.014675 , Elapsed time  60.81  minutes.\n",
    "Iteration: 8600. Average reward per game:  -0.012906 , Elapsed time  61.65  minutes.\n",
    "Iteration: 8700. Average reward per game:  -0.016207 , Elapsed time  62.31  minutes.\n",
    "Iteration: 8800. Average reward per game:  -0.014398 , Elapsed time  62.98  minutes.\n",
    "Iteration: 8900. Average reward per game:  -0.010011 , Elapsed time  63.7  minutes.\n",
    "Iteration: 9000. Average reward per game:  -0.015393 , Elapsed time  64.46  minutes.\n",
    "Iteration: 9100. Average reward per game:  -0.015236 , Elapsed time  65.22  minutes.\n",
    "Iteration: 9200. Average reward per game:  -0.01523 , Elapsed time  65.98  minutes.\n",
    "Iteration: 9300. Average reward per game:  -0.010458 , Elapsed time  66.68  minutes.\n",
    "Iteration: 9400. Average reward per game:  -0.012036 , Elapsed time  67.44  minutes.\n",
    "Iteration: 9500. Average reward per game:  -0.009933 , Elapsed time  68.18  minutes.\n",
    "Iteration: 9560Saved Model with average discounted rewards per game  -0.00634955555297746\n",
    "Iteration: 9600. Average reward per game:  -0.017751 , Elapsed time  69.0  minutes.\n",
    "Iteration: 9700. Average reward per game:  -0.02269 , Elapsed time  69.82  minutes.\n",
    "Iteration: 9800. Average reward per game:  -0.011657 , Elapsed time  70.54  minutes.\n",
    "Iteration: 9900. Average reward per game:  -0.012043 , Elapsed time  71.31  minutes.\n",
    "Iteration: 10000. Average reward per game:  -0.012472 , Elapsed time  72.16  minutes.\n",
    "Iteration: 10100. Average reward per game:  -0.017726 , Elapsed time  73.04  minutes.\n",
    "Iteration: 10200. Average reward per game:  -0.017362 , Elapsed time  73.8  minutes.\n",
    "Iteration: 10300. Average reward per game:  -0.026485 , Elapsed time  74.46  minutes.\n",
    "Iteration: 10400. Average reward per game:  -0.018958 , Elapsed time  75.13  minutes.\n",
    "Iteration: 10500. Average reward per game:  -0.008506 , Elapsed time  75.93  minutes.\n",
    "Iteration: 10600. Average reward per game:  -0.01576 , Elapsed time  76.68  minutes.\n",
    "Iteration: 10700. Average reward per game:  -0.008541 , Elapsed time  77.32  minutes.\n",
    "Iteration: 10800. Average reward per game:  -0.008042 , Elapsed time  78.09  minutes.\n",
    "Iteration: 10900. Average reward per game:  -0.014927 , Elapsed time  78.74  minutes.\n",
    "Iteration: 11000. Average reward per game:  -0.008022 , Elapsed time  79.4  minutes.\n",
    "Iteration: 11100. Average reward per game:  -0.012763 , Elapsed time  80.18  minutes.\n",
    "Iteration: 11160Saved Model with average discounted rewards per game  -0.004349204311752651\n",
    "Iteration: 11200. Average reward per game:  -0.012041 , Elapsed time  80.85  minutes.\n",
    "Iteration: 11300. Average reward per game:  -0.012542 , Elapsed time  81.51  minutes.\n",
    "Iteration: 11400. Average reward per game:  -0.012829 , Elapsed time  82.17  minutes.\n",
    "Iteration: 11500. Average reward per game:  -0.008957 , Elapsed time  82.82  minutes.\n",
    "Iteration: 11600. Average reward per game:  -0.032163 , Elapsed time  83.48  minutes.\n",
    "Iteration: 11700. Average reward per game:  -0.024923 , Elapsed time  84.24  minutes.\n",
    "Iteration: 11800. Average reward per game:  -0.033403 , Elapsed time  84.89  minutes.\n",
    "Iteration: 11900. Average reward per game:  -0.032755 , Elapsed time  85.56  minutes.\n",
    "Iteration: 12000. Average reward per game:  -0.018305 , Elapsed time  86.2  minutes.\n",
    "Iteration: 12100. Average reward per game:  -0.025577 , Elapsed time  86.88  minutes.\n",
    "Iteration: 12200. Average reward per game:  -0.020394 , Elapsed time  87.58  minutes.\n",
    "Iteration: 12300. Average reward per game:  -0.011537 , Elapsed time  88.24  minutes.\n",
    "Iteration: 12400. Average reward per game:  -0.015151 , Elapsed time  88.91  minutes.\n",
    "Iteration: 12500. Average reward per game:  -0.017093 , Elapsed time  89.57  minutes.\n",
    "Iteration: 12600. Average reward per game:  -0.013253 , Elapsed time  90.24  minutes.\n",
    "Iteration: 12700. Average reward per game:  -0.011306 , Elapsed time  90.89  minutes.\n",
    "Iteration: 12800. Average reward per game:  -0.012063 , Elapsed time  91.53  minutes.\n",
    "Iteration: 12900. Average reward per game:  -0.011635 , Elapsed time  92.28  minutes.\n",
    "Iteration: 13000. Average reward per game:  -0.005359 , Elapsed time  92.92  minutes.\n",
    "Iteration: 13100. Average reward per game:  -0.004771 , Elapsed time  93.63  minutes.\n",
    "Iteration: 13200. Average reward per game:  -0.008373 , Elapsed time  94.28  minutes.\n",
    "Iteration: 13300. Average reward per game:  -0.010142 , Elapsed time  95.02  minutes.\n",
    "Iteration: 13400. Average reward per game:  -0.017111 , Elapsed time  95.69  minutes.\n",
    "Iteration: 13500. Average reward per game:  -0.007101 , Elapsed time  96.35  minutes.\n",
    "Iteration: 13600. Average reward per game:  -0.011773 , Elapsed time  96.98  minutes.\n",
    "Iteration: 13700. Average reward per game:  -0.018028 , Elapsed time  97.66  minutes.\n",
    "Iteration: 13800. Average reward per game:  -0.019281 , Elapsed time  98.34  minutes.\n",
    "Iteration: 13900. Average reward per game:  -0.009676 , Elapsed time  99.0  minutes.\n",
    "Iteration: 14000. Average reward per game:  -0.01622 , Elapsed time  99.66  minutes.\n",
    "Iteration: 14100. Average reward per game:  -0.01009 , Elapsed time  100.31  minutes.\n",
    "Iteration: 14200. Average reward per game:  -0.007395 , Elapsed time  100.93  minutes.\n",
    "Iteration: 14300. Average reward per game:  -0.011872 , Elapsed time  101.66  minutes.\n",
    "Iteration: 14400. Average reward per game:  -0.018928 , Elapsed time  102.33  minutes.\n",
    "Iteration: 14500. Average reward per game:  -0.012237 , Elapsed time  103.03  minutes.\n",
    "Iteration: 14600. Average reward per game:  -0.01168 , Elapsed time  103.69  minutes.\n",
    "Iteration: 14700. Average reward per game:  -0.02245 , Elapsed time  104.39  minutes.\n",
    "Iteration: 14800. Average reward per game:  -0.012821 , Elapsed time  105.07  minutes.\n",
    "Iteration: 14900. Average reward per game:  -0.015703 , Elapsed time  105.81  minutes.\n",
    "Iteration: 15000. Average reward per game:  -0.009862 , Elapsed time  106.49  minutes.\n",
    "Iteration: 15100. Average reward per game:  -0.009115 , Elapsed time  107.15  minutes.\n",
    "Iteration: 15200. Average reward per game:  -0.011161 , Elapsed time  107.81  minutes.\n",
    "Iteration: 15300. Average reward per game:  -0.01204 , Elapsed time  108.52  minutes.\n",
    "Iteration: 15400. Average reward per game:  -0.010677 , Elapsed time  109.21  minutes.\n",
    "Iteration: 15500. Average reward per game:  -0.026854 , Elapsed time  110.41  minutes.\n",
    "Iteration: 15600. Average reward per game:  -0.014051 , Elapsed time  111.09  minutes.\n",
    "Iteration: 15700. Average reward per game:  -0.012619 , Elapsed time  111.76  minutes.\n",
    "Iteration: 15800. Average reward per game:  -0.020738 , Elapsed time  112.41  minutes.\n",
    "Iteration: 15900. Average reward per game:  -0.014535 , Elapsed time  113.35  minutes.\n",
    "Iteration: 16000. Average reward per game:  -0.015573 , Elapsed time  114.48  minutes.\n",
    "Iteration: 16100. Average reward per game:  -0.009438 , Elapsed time  115.15  minutes.\n",
    "Iteration: 16200. Average reward per game:  -0.012786 , Elapsed time  115.91  minutes.\n",
    "Iteration: 16300. Average reward per game:  -0.014183 , Elapsed time  116.56  minutes.\n",
    "Iteration: 16400. Average reward per game:  -0.009544 , Elapsed time  117.25  minutes.\n",
    "Iteration: 16500. Average reward per game:  -0.012073 , Elapsed time  118.05  minutes.\n",
    "Iteration: 16600. Average reward per game:  -0.010683 , Elapsed time  118.92  minutes.\n",
    "Iteration: 16700. Average reward per game:  -0.009347 , Elapsed time  119.78  minutes.\n",
    "Iteration: 16800. Average reward per game:  -0.01654 , Elapsed time  120.59  minutes.\n",
    "Iteration: 16900. Average reward per game:  -0.016944 , Elapsed time  121.34  minutes.\n",
    "Iteration: 17000. Average reward per game:  -0.014735 , Elapsed time  122.03  minutes.\n",
    "Iteration: 17100. Average reward per game:  -0.019304 , Elapsed time  122.76  minutes.\n",
    "Iteration: 17167\n",
    "---------------------------------------------------------------------------\n",
    "KeyboardInterrupt                         Traceback (most recent call last)\n",
    "<ipython-input-7-035f1fc36fd9> in <module>()\n",
    "     51             obs = env.reset()\n",
    "     52             for step in range(n_max_steps):\n",
    "---> 53                 action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)})\n",
    "     54                 vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "     55                 obs, reward, done, info = env.step(vect_action)\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n",
    "    903     try:\n",
    "    904       result = self._run(None, fetches, feed_dict, options_ptr,\n",
    "--> 905                          run_metadata_ptr)\n",
    "    906       if run_metadata:\n",
    "    907         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n",
    "   1078           try:\n",
    "   1079             subfeed_t = self.graph.as_graph_element(\n",
    "-> 1080                 subfeed, allow_tensor=True, allow_operation=False)\n",
    "   1081           except Exception as e:\n",
    "   1082             raise TypeError(\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\n",
    "   3476 \n",
    "   3477     with self._lock:\n",
    "-> 3478       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
    "   3479 \n",
    "   3480   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\n",
    "\n",
    "KeyboardInterrupt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration: 0. Average reward per game:  -1.937177 , Elapsed time  0.01  minutes.\n",
    "Saved Model with average discounted rewards per game  -1.9371774844162275\n",
    "Iteration: 20Saved Model with average discounted rewards per game  -0.9706484292856778\n",
    "Iteration: 40Saved Model with average discounted rewards per game  -0.7654785649834133\n",
    "Iteration: 60Saved Model with average discounted rewards per game  -0.5995210797681685\n",
    "Iteration: 80Saved Model with average discounted rewards per game  -0.4327503840177901\n",
    "Iteration: 100. Average reward per game:  -0.386753 , Elapsed time  0.66  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.3867534294308506\n",
    "Iteration: 120Saved Model with average discounted rewards per game  -0.35601938508922715\n",
    "Iteration: 140Saved Model with average discounted rewards per game  -0.27318779173276975\n",
    "Iteration: 160Saved Model with average discounted rewards per game  -0.23106525838237135\n",
    "Iteration: 200. Average reward per game:  -0.20511 , Elapsed time  1.31  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.2051098850535659\n",
    "Iteration: 220Saved Model with average discounted rewards per game  -0.19273440501698522\n",
    "Iteration: 240Saved Model with average discounted rewards per game  -0.17994594949574835\n",
    "Iteration: 300. Average reward per game:  -0.202603 , Elapsed time  1.96  minutes.\n",
    "Iteration: 400. Average reward per game:  -0.15286 , Elapsed time  2.61  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.15285980174208236\n",
    "Iteration: 420Saved Model with average discounted rewards per game  -0.14959204211641602\n",
    "Iteration: 500. Average reward per game:  -0.180208 , Elapsed time  3.26  minutes.\n",
    "Iteration: 600. Average reward per game:  -0.164707 , Elapsed time  3.92  minutes.\n",
    "Iteration: 700. Average reward per game:  -0.204476 , Elapsed time  4.56  minutes.\n",
    "Iteration: 720Saved Model with average discounted rewards per game  -0.14724224064318006\n",
    "Iteration: 800. Average reward per game:  -0.182793 , Elapsed time  5.21  minutes.\n",
    "Iteration: 900. Average reward per game:  -0.137978 , Elapsed time  5.89  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.13797844973419546\n",
    "Iteration: 1000. Average reward per game:  -0.14535 , Elapsed time  6.57  minutes.\n",
    "Iteration: 1060Saved Model with average discounted rewards per game  -0.13583961971086284\n",
    "Iteration: 1100. Average reward per game:  -0.134264 , Elapsed time  7.29  minutes.\n",
    "Saved Model with average discounted rewards per game  -0.13426393970402908\n",
    "Iteration: 1200. Average reward per game:  -0.175209 , Elapsed time  8.0  minutes.\n",
    "Iteration: 1300. Average reward per game:  -0.135512 , Elapsed time  8.71  minutes.\n",
    "Iteration: 1400. Average reward per game:  -0.160529 , Elapsed time  9.43  minutes.\n",
    "Iteration: 1460Saved Model with average discounted rewards per game  -0.13380996068458037\n",
    "Iteration: 1480Saved Model with average discounted rewards per game  -0.12620859370521165\n",
    "Iteration: 1500. Average reward per game:  -0.163708 , Elapsed time  10.11  minutes.\n",
    "Iteration: 1600. Average reward per game:  -0.16455 , Elapsed time  10.78  minutes.\n",
    "Iteration: 1700. Average reward per game:  -0.206061 , Elapsed time  11.45  minutes.\n",
    "Iteration: 1800. Average reward per game:  -0.131185 , Elapsed time  12.1  minutes.\n",
    "Iteration: 1900. Average reward per game:  -0.146679 , Elapsed time  12.74  minutes.\n",
    "Iteration: 2000. Average reward per game:  -0.164523 , Elapsed time  13.42  minutes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "724px",
    "left": "37px",
    "top": "105px",
    "width": "343px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
